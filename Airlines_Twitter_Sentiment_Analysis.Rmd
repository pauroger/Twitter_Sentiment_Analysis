# Twitter Sentiment Analysis, NLP

Pau Roger Puig-Sureda

28/03/2018


## Goal

Predict if tweets have a negative, neutral or positive sentiment towards several airlines.

### Import libraries

```{r, message=FALSE, warning=FALSE}
library(tm)
library(e1071)
library(SnowballC)
library(caret)
library(plyr)
library(kernlab)
library(magrittr)
library(ggplot2)
library(naivebayes)
library(data.table)
```

### Read Data

```{r, warning=FALSE}
setwd("C:/Users/usuario/Documents/Term2_Local/Machine Learning II/Assignment 3")
train <- read.csv(file = file.path("training.csv"), stringsAsFactors = TRUE)
test <- read.csv(file = file.path("test.csv"), stringsAsFactors = TRUE)
```

### Join datasets

Train has 1 extra column

```{r merge}
# Sum of NA's
sum(is.na(train$X.5))
count(train$X.5) # This row is useless, I will delete it.
train$X.5 <- NULL
# Next, I am adding the target column 'airline_sentiment' into the test dataset.
test$airline_sentiment <- as.factor(NA)
joined <- rbind(train, test)
# Change factors to numbers of the target variable
joined$airline_sentiment <- as.factor(unclass(joined$airline_sentiment) %>% as.numeric)
```

### Check for NA's

```{r NAs}
na.cols <- which(colSums(is.na(joined)) > 0)
sort(colSums(sapply(joined[na.cols], is.na)), decreasing = TRUE)
```

There are none.

### Variable check

```{r vars}
# head(joined$name) # Username
# summary(joined$airline_sentiment)
# summary(joined$airline) # American / Delta / Southwest / United / US Airways Virgin America
# summary(joined$retweet_count) # Mostly none, so not useful.
# summary(joined$tweet_coord) # Mix of actuall coordinates and text.
# summary(joined$tweet_created) # DateTime.
# summary(joined$tweet_location) # Locations, there are a bunch of typos.
# summary(joined$user_timezone)
# summary(joined$X) # Mic of location, datetime, coordinates...
# summary(joined$X.1)
# summary(joined$X.4)
```

### Visualization

Count of tweets per sentiment.

```{r visualization}
sentiment <- as.data.frame(count(train$airline_sentiment))
colnames(sentiment) <- c("Sentiment","Count")
ggplot(sentiment) + aes(x=Sentiment, y=Count, fill=Sentiment) + geom_bar(stat = 'identity') + scale_fill_brewer(palette="Spectral")
```

We can see that when people tweet about an airline, most of the time is to complain about it. Good service is mainly assumed.

Count of twets per sentiment and grouped by airline.

```{r visualization2}
A_sentiment = as.data.frame(table(train$airline,train$airline_sentiment))
colnames(A_sentiment) = c("Company","Sentiment","Count")
ggplot(A_sentiment) + aes(x=Company, y=Count, fill=Sentiment) + geom_bar(stat = 'identity') + scale_fill_brewer(palette="Spectral")
```

'United' is the airline with the most negative tweets, followed by 'US Airways' and 'American'.

### Tweet content

First thing to do is reading the text in the 'tweets'.

```{r, echo=TRUE}
corpus <- Corpus(VectorSource(joined$text))
# We can take a look at some examples.
inspect(corpus[1:5])
```

Next, with this function, I will:

  * stemming
  
  * remove punctuation
  
  * strip blanks
  
  * remove stopwords
  
  * remove links

```{r function}
cleanCorpus <- function(corpus) {
  corpus <-tm_map(corpus, stemDocument)
  corpus.tmp <- tm_map(corpus,removePunctuation)
  corpus.tmp <- tm_map(corpus.tmp,stripWhitespace)
  corpus.tmp <- tm_map(corpus.tmp,removeWords,stopwords("en"))
  corpus.tmp <- tm_map(corpus, function(x) gsub('http[[:alnum:]]*', '', x))
  return(corpus.tmp)
}
```

Clean the 'corpus' variable.

```{r clean}
corpus.clean <- cleanCorpus(corpus)
```

Represent the bag of words tokens with a document term matrix (DTM). The rows of the DTM will correspond to the documents in the collection, columns to the terms, and its elements are the term frequencies.

```{r dtm}
dtm <- DocumentTermMatrix(corpus.clean)
dtm
```

Countplot of the words that appear more than 600 times (this number is set so that the graph remains readable).

```{r graph count}
count <- colSums(as.matrix(dtm)) 
word <- order(count)
df = data.frame(word=names(count), count=count)   

ggplot(subset(df, count>600), aes(word, count, fill = count)) + geom_bar(stat="identity", show.legend = TRUE) + theme(axis.text.x=element_text(angle=45, hjust=1, size = rel(0.7)))
```

Shape of the newly created 'bag of words'.

```{r shape}
dim(dtm)
```

Remove the sparse terms in the matrix (i.e., those terms only appearing in a few reviews)

```{r dtm2}
dtm.nb <- removeSparseTerms(dtm, 0.99)
```

### Converting to a DataFrame

```{r df}
tweets <- as.data.frame(as.matrix(dtm.nb))
colnames(tweets) <- make.names(colnames(tweets))
tweets$airline_sentiment <- joined$airline_sentiment
dim(tweets)
```

### Training and Test data sets.

Split again training and test

```{r}
# Only text data
ctrain <- tweets[1:7000,]
ctest <- tweets[7001:14640,]

# As we saw on the graph before, depending on the airline the % of a tweet being positive, negative or neutral varies. So, I will now join that info to the text.
binarizing_airline <- cbind(joined, sapply(levels(joined$airline), function(x) as.integer(x == joined$airline)), joined[1])
binarizing_airline <- binarizing_airline[,c(16:21)]

plus_airline <- cbind(binarizing_airline, tweets)
plus_airline_train <- plus_airline[1:7000,]
plus_airline_test <- plus_airline[7001:14640,]
```

#### What if we make a model that says ALL negative?

```{r percent}
percentages = as.data.frame(prop.table(table(ctrain$airline_sentiment)))
colnames(percentages) = c('Sentiment', '%')
percentages
```

It would get it right 62.5% of the time. We need to improve this number by using different models.

### SVM

```{r message = FALSE, warning=FALSE}
#train control for cross validation
train_control<- trainControl(method="cv", number=5,  search="random", verboseIter = TRUE)

# SVM cross validation
cv.svm_<- train(as.factor(airline_sentiment)~., data=data.matrix(ctrain),
                 trControl=train_control,
                 method="svmRadial", # SVM Radial Kernel
                 metric="Accuracy",
                 tuneGrid= expand.grid(sigma=c(0.02, 0.01), C=c(1, 2)),
                 verbose=TRUE)
``` 

```{r SVM results}
cv.svm_$results
``` 

73.6% accuracy, I will now try with the training dataset: 'plus_airline_train'.

```{r message = FALSE, warning=FALSE}
# SVM cross validation
cv.svm<- train(as.factor(airline_sentiment)~., data=data.matrix(plus_airline_train), 
                 trControl=train_control,
                 method="svmRadial", # SVM Radial Kernel
                 metric="Accuracy",
                 tuneGrid= expand.grid(sigma=c(0.02, 0.01), C=c(2, 1)), 
                 verbose=TRUE)
``` 

```{r SVM results2}
cv.svm$results
``` 

73.7%. Seeing that there is not an improvement, I will use the 'ctrain' because it is simplier than the other one.

### Naive Bayes

```{r message = FALSE, warning=FALSE}
grid <- data.frame(laplace=c(0, 0.5), usekernel = TRUE, adjust=c(0, 0.5))

cv.nb<- train(as.factor(airline_sentiment)~., data=data.matrix(ctrain), 
                 trControl=train_control,
                 method="naive_bayes",
                 metric="Accuracy",
                 tuneGrid= grid,
                 verbose=TRUE)
```

```{r nb results}
cv.nb$results
```

21,9%. Results with Naive Bayes are really poor. Probably the independance assuption does not apply in this case.

### Random Forest

```{r message = FALSE, warning=FALSE}
cv.rf<- train(as.factor(airline_sentiment)~., data=data.matrix(ctrain),
                 trControl=train_control,
                 method="rf", # Random Forest
                 metric="Accuracy",
                 tuneGrid= expand.grid(.mtry=c(7, 8, 9, 10)), # Number of features to be used for the random forest model
                 verbose=TRUE)
```

```{r results rf}
cv.rf$results
```

74.4% An improvement on the previous models.

### XGBoost

```{r message = FALSE, warning=FALSE}
# Only keepts the best parameters of the previous iterations to speed up the computation process.
tuneGridXGB <- expand.grid(
    nrounds=c(150),
    max_depth = c(25),
    eta = c(0.02),
    gamma = c(1),
    colsample_bytree = c(0.75),
    subsample = c(0.75),
    min_child_weight = c(2))

# train the xgboost learner
cv.xgboost <- train(as.factor(airline_sentiment)~., data=data.matrix(ctrain),
    method = 'xgbTree',
    metric = 'Accuracy',
    trControl = train_control,
    tuneGrid = tuneGridXGB)
```

```{r results XGBoost}
cv.xgboost$results
```

74.8% Best one so far!

## KNN

```{r message = FALSE, warning=FALSE}
cv.knn <- train(as.factor(airline_sentiment)~., data=data.matrix(ctrain),
                method = "knn",
                trControl = train_control,
                preProcess = c("center","scale"),
                tuneLength = 20,
                tuneGrid= expand.grid(.k=c(15, 20, 18)))
```

```{r KNN results}
cv.knn$results
```

Finally, K-Nearest Neighbours has an accuracy of 61.7%.

### Final Submission

I Choose to create a csv file only for the model with the best prediction on the training set, XGBoost.

```{r submission}
submission_set = read.csv("C:/Users/usuario/Documents/Term2_Local/Machine Learning II/Assignment 3/sample_submission.csv", header = TRUE, sep = ",")
pred <- predict(cv.xgboost, data.matrix(ctest))
submission <- data.frame(tweet_id = submission_set$tweet_id, y = pred)

# Change the numbers back to the letter-initial factors.
submission$airline_sentiment[submission$y == 1] <- "negative"
submission$airline_sentiment[submission$y == 2] <- "neutral"
submission$airline_sentiment[submission$y == 3] <- "positive"

# Save the output in a csv file in the required format.
write.csv(submission[ , which(names(submission) %in% c("tweet_id", "airline_sentiment"))], file = "Assignment3_Pau_Roger.csv", row.names=FALSE, quote = FALSE)
```


